"""–°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è ai_library_bot.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç FAISS –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞
–Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
"""

from typing import Any

from src.config import Config
from src.utils import setup_logger

logger = setup_logger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
NOT_FOUND = "NOT_FOUND"


async def get_retriever() -> Any:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç retriever (FAISS –∏–Ω–¥–µ–∫—Å).

    –ó–∞–≥—Ä—É–∂–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–∞.

    Returns:
        FAISS –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.

    Raises:
        FileNotFoundError: –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω.
    """
    import faiss
    import pickle

    index_path = Config.FAISS_PATH
    metadata_path = index_path.with_suffix(".metadata.pkl")

    if not index_path.exists():
        raise FileNotFoundError(
            f"FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_path}. "
            f"–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É ingest –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞."
        )

    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ {index_path}")
    index = faiss.read_index(str(index_path))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = []
    if metadata_path.exists():
        with open(metadata_path, "rb") as f:
            metadata = pickle.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(metadata)} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ book2.txt –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        book2_metadata = [m for m in metadata if m.get("source") == "book2.txt"]
        logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ book2.txt: {len(book2_metadata)}")
        
        for i, meta in enumerate(book2_metadata[:3]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞ –∏–∑ book2.txt
            chunk_text = meta.get("chunk_text", "")
            chunk_idx = meta.get("chunk_index", i)
            if chunk_text:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                preview = chunk_text[:100]
                logger.info(f"[–ü–†–û–í–ï–†–ö–ê] –ß–∞–Ω–∫ {chunk_idx} –∏–∑ book2.txt, –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤: {preview}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã
                unreadable = sum(1 for c in preview if ord(c) > 127 and not c.isprintable() and c not in "\n\r\t" and c not in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø")
                if unreadable > len(preview) * 0.1:
                    logger.warning(f"[–ü–†–û–í–ï–†–ö–ê] ‚ö†Ô∏è –ß–∞–Ω–∫ {chunk_idx} –∏–∑ book2.txt —Å–æ–¥–µ—Ä–∂–∏—Ç –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã: {unreadable} –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(preview)}")
                else:
                    logger.info(f"[–ü–†–û–í–ï–†–ö–ê] ‚úÖ –ß–∞–Ω–∫ {chunk_idx} –∏–∑ book2.txt –≤—ã–≥–ª—è–¥–∏—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
    
    logger.info(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {index.d}")
    
    return {"index": index, "metadata": metadata}


async def _create_query_embedding(query: str) -> list[float]:
    """–°–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Returns:
        –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (—Å–ø–∏—Å–æ–∫ float).

    Raises:
        ValueError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥.
    """
    if not Config.OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
    
    logger.info(f"[RETRIEVER] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ OpenAI API")
    logger.debug(f"[RETRIEVER] –ó–∞–ø—Ä–æ—Å: {query[:100]}...")
    logger.info(f"[RETRIEVER] –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {Config.EMBEDDING_MODEL}")
    
    try:
        response = await client.embeddings.create(
            model=Config.EMBEDDING_MODEL,
            input=query
        )
        embedding = response.data[0].embedding
        logger.info(f"[RETRIEVER] ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(embedding)}")
        logger.debug(f"[RETRIEVER] –ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {embedding[:5]}")
        return embedding
    except Exception as e:
        logger.error(f"[RETRIEVER] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {type(e).__name__}: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥: {e}") from e


async def _search_in_faiss(
    retriever: Any, query_embedding: list[float], top_k: int, query: str = ""
) -> list[tuple[Any, float]]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ.

    Args:
        retriever: Retriever –æ–±—ä–µ–∫—Ç —Å FAISS –∏–Ω–¥–µ–∫—Å–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
        query_embedding: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞.
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞.
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏).

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (chunk_data, score), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
    """
    import numpy as np

    index = retriever["index"]
    metadata = retriever["metadata"]

    logger.info(f"[RETRIEVER] –ü–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ: top_k={top_k}, –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {index.ntotal}")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –≤ numpy array
    query_vector = np.array([query_embedding], dtype=np.float32)
    logger.debug(f"[RETRIEVER] Query vector shape: {query_vector.shape}, dtype: {query_vector.dtype}")

    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º top_k –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ)
    actual_top_k = min(top_k, index.ntotal)
    logger.info(f"[RETRIEVER] –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ (actual_top_k={actual_top_k}, –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ={index.ntotal})...")
    
    # –î–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: –∏—â–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –µ—Å—Ç—å –ª–∏ —á–∞–Ω–∫–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    search_k = min(actual_top_k * 2, index.ntotal)  # –ò—â–µ–º –≤ 2 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    distances, indices = index.search(query_vector, search_k)
    logger.info(f"[RETRIEVER] –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(indices[0])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∏—Å–∫–∞–ª–∏ {search_k})")
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –ø–æ–∏—Å–∫–µ
    extended_sources = {}
    for idx in indices[0]:
        if idx >= 0 and idx < len(metadata):
            source = metadata[idx].get("source", "unknown")
            extended_sources[source] = extended_sources.get(source, 0) + 1
    logger.info(f"[RETRIEVER] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –ø–æ–∏—Å–∫–µ (—Ç–æ–ø-{search_k}): {extended_sources}")

    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ actual_top_k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    results = []
    for i, (dist, idx) in enumerate(zip(distances[0][:actual_top_k], indices[0][:actual_top_k])):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ score (—á–µ–º –º–µ–Ω—å—à–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ç–µ–º –≤—ã—à–µ score)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–æ—Ä–º—É–ª—É: score = 1 / (1 + distance)
        distance = float(dist)
        score = 1.0 / (1.0 + distance)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
        if idx < len(metadata) and idx >= 0:
            chunk_meta = metadata[idx]
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            chunk_text = chunk_meta.get("chunk_text", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã
            if chunk_text and isinstance(chunk_text, str):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∞–∫–æ–∑—è–±—Ä
                preview = chunk_text[:100]
                # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 30% —Å–∏–º–≤–æ–ª–æ–≤ - –Ω–µ—á–∏—Ç–∞–µ–º—ã–µ (–Ω–µ –±—É–∫–≤—ã, –Ω–µ —Ü–∏—Ñ—Ä—ã, –Ω–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, –Ω–µ –ø—Ä–æ–±–µ–ª—ã, –Ω–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
                unreadable = sum(1 for c in preview if ord(c) > 127 and not c.isprintable() and c not in "\n\r\t" and c not in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø")
                if unreadable > len(preview) * 0.3:
                    logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã –≤ —á–∞–Ω–∫–µ {idx}, –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É...")
                    # –ü—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –∫–∞–∫ latin-1 –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–∞–∫ cp1251
                    try:
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –±—ã–ª –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
                        chunk_text = chunk_text.encode("latin-1", errors="ignore").decode("cp1251", errors="replace")
                        logger.info(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞ —á–∞–Ω–∫–∞ {idx} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞")
                    except (UnicodeEncodeError, UnicodeDecodeError):
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É —á–∞–Ω–∫–∞ {idx}")
                        pass
            # –ü–æ–ª—É—á–∞–µ–º source –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ title –∏–ª–∏ file_path)
            source = chunk_meta.get("source", "unknown")
            if source == "unknown":
                # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ title –∏–ª–∏ file_path
                title = chunk_meta.get("title", "")
                file_path = chunk_meta.get("file_path", "")
                if title:
                    source = title
                elif file_path:
                    from pathlib import Path
                    source = Path(file_path).name
                else:
                    source = "unknown"
            
            chunk_data = {
                "text": chunk_text,
                "source": source,
                "chunk_index": chunk_meta.get("chunk_index", idx),
            }
            logger.debug(f"[RETRIEVER] –ß–∞–Ω–∫ {i+1} (idx={idx}): source={chunk_data['source']}, text_length={len(chunk_text)}")
            logger.debug(f"[RETRIEVER] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞ {idx}: {list(chunk_meta.keys())}")
        else:
            # Fallback, –µ—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ idx < 0 (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å –æ—Ç FAISS)
            if idx < 0:
                logger.warning(f"[RETRIEVER] ‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å –æ—Ç FAISS: {idx} (–≤–æ–∑–º–æ–∂–Ω–æ, –≤ –∏–Ω–¥–µ–∫—Å–µ –º–µ–Ω—å—à–µ –≤–µ–∫—Ç–æ—Ä–æ–≤, —á–µ–º –∑–∞–ø—Ä–æ—à–µ–Ω–æ)")
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
            chunk_data = {
                "text": f"–ß–∞–Ω–∫ {idx}",
                "source": "unknown",
                "chunk_index": idx,
            }
            logger.warning(f"[RETRIEVER] ‚ö†Ô∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {idx} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–≤—Å–µ–≥–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(metadata)})")
        
        results.append((chunk_data, score))
        logger.info(
            f"[RETRIEVER] –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: idx={idx}, distance={distance:.4f}, "
            f"score={score:.4f}, source={chunk_data.get('source', 'unknown')}, "
            f"text_preview={chunk_data.get('text', '')[:80]}..."
        )

    return results


def _filter_by_score(results: list[tuple[Any, float]], threshold: float) -> list[tuple[Any, float]]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.

    Args:
        results: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (chunk_data, score).
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.

    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    filtered = [(chunk, score) for chunk, score in results if score >= threshold]
    logger.debug(
        f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)} ‚Üí {len(filtered)} " f"(threshold={threshold})"
    )
    return filtered


def _apply_smart_filtering(results: list[tuple[Any, float]]) -> list[tuple[Any, float]]:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —É–º–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤.

    –ï—Å–ª–∏ —Ç–æ–ø-N —á–∞–Ω–∫–æ–≤ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π score (> –ø–æ—Ä–æ–≥–∞), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –∏—Ö.
    –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

    Args:
        results: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (chunk_data, score), –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é score.

    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    """
    if not Config.SMART_FILTERING_ENABLED:
        return results
    
    if len(results) == 0:
        return results
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ø-N —á–∞–Ω–∫–æ–≤
    top_n = min(Config.SMART_FILTERING_TOP_N, len(results))
    top_chunks = results[:top_n]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ —Ç–æ–ø-N —á–∞–Ω–∫–æ–≤ –∏–º–µ—é—Ç score –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
    all_high_score = all(
        score >= Config.SMART_FILTERING_SCORE_THRESHOLD 
        for _, score in top_chunks
    )
    
    if all_high_score:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N —á–∞–Ω–∫–æ–≤
        logger.info(
            f"[RETRIEVER] üéØ –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Ç–æ–ø-{top_n} —á–∞–Ω–∫–æ–≤ –∏–º–µ—é—Ç score >= {Config.SMART_FILTERING_SCORE_THRESHOLD}, "
            f"–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö (—ç–∫–æ–Ω–æ–º–∏—è: {len(results) - top_n} —á–∞–Ω–∫–æ–≤)"
        )
        return top_chunks
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(
            f"[RETRIEVER] üìä –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –Ω–µ –≤—Å–µ —Ç–æ–ø-{top_n} —á–∞–Ω–∫–æ–≤ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–∏–π score, "
            f"–∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        )
        return results


async def retrieve_chunks(query: str) -> list[dict[str, Any]] | str:
    """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –°–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    2. –ò—â–µ—Ç –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ top-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    3. –§–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (score >= 0.7)
    4. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–ª–∏ NOT_FOUND

    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ NOT_FOUND,
        –µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç.

        –§–æ—Ä–º–∞—Ç —á–∞–Ω–∫–∞:
        {
            "text": "—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞",
            "source": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–Ω–∏–≥–∏.txt",
            "chunk_index": 0,
            "score": 0.85
        }
    """
    if not query or not query.strip():
        logger.warning("[RETRIEVER] –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")
        return NOT_FOUND

    logger.info(f"[RETRIEVER] ===== –ù–∞—á–∞–ª–æ –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ =====")
    logger.info(f"[RETRIEVER] –ó–∞–ø—Ä–æ—Å: {query}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è retriever
    logger.info(f"[RETRIEVER] –≠—Ç–∞–ø 1/3: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è retriever (–∑–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞)")
    retriever = await get_retriever()

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
    logger.info(f"[RETRIEVER] –≠—Ç–∞–ø 2/3: –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ OpenAI API")
    query_embedding = await _create_query_embedding(query)
    logger.info(f"[RETRIEVER] ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(query_embedding)}")

    # –ü–æ–∏—Å–∫ –≤ FAISS
    logger.info(f"[RETRIEVER] –≠—Ç–∞–ø 3/3: –ü–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ (top_k={Config.TOP_K})")
    results = await _search_in_faiss(retriever, query_embedding, top_k=Config.TOP_K)

    # –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
    if Config.SMART_FILTERING_ENABLED:
        results = _apply_smart_filtering(results)
        logger.info(f"[RETRIEVER] –ü–æ—Å–ª–µ —É–º–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    logger.info(f"[RETRIEVER] –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
    logger.info(f"[RETRIEVER] –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (SCORE_THRESHOLD): {Config.SCORE_THRESHOLD}")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    sources_count = {}
    for chunk_data, score in results:
        source = chunk_data.get('source', 'unknown')
        sources_count[source] = sources_count.get(source, 0) + 1
    logger.info(f"[RETRIEVER] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º: {sources_count}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏—Ö score
    for i, (chunk_data, score) in enumerate(results):
        logger.info(
            f"[RETRIEVER] –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: score={score:.4f}, "
            f"source={chunk_data.get('source', 'unknown')}, "
            f"text_preview={chunk_data.get('text', '')[:100]}..."
        )
    
    filtered_results = _filter_by_score(results, Config.SCORE_THRESHOLD)
    logger.info(f"[RETRIEVER] –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filtered_sources_count = {}
    for chunk_data, score in filtered_results:
        source = chunk_data.get('source', 'unknown')
        filtered_sources_count[source] = filtered_sources_count.get(source, 0) + 1
    logger.info(f"[RETRIEVER] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {filtered_sources_count}")

    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –±–µ—Ä—ë–º –ª—É—á—à–∏–µ
    if not filtered_results and results:
        logger.warning(
            f"[RETRIEVER] ‚ö†Ô∏è –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã (score threshold={Config.SCORE_THRESHOLD}). "
            f"–ë–µ—Ä—ë–º —Ç–æ–ø-{min(3, len(results))} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ score."
        )
        # –ë–µ—Ä—ë–º —Ç–æ–ø-3 —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ score, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–∏–∂–µ threshold
        filtered_results = sorted(results, key=lambda x: x[1], reverse=True)[:3]
        logger.info(f"[RETRIEVER] –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(filtered_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–∞–∏–ª—É—á—à–∏–º–∏ score:")
        for i, (chunk_data, score) in enumerate(filtered_results):
            logger.info(f"[RETRIEVER]   - –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: score={score:.4f}, source={chunk_data.get('source', 'unknown')}")
    
    # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if not filtered_results:
        logger.warning(
            f"[RETRIEVER] ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ "
            f"(score threshold={Config.SCORE_THRESHOLD}, –≤—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)})"
        )
        logger.info(f"[RETRIEVER] ===== –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω: NOT_FOUND =====")
        return NOT_FOUND

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    chunks = []
    for chunk_data, score in filtered_results:
        chunk = {
            "text": chunk_data.get("text", ""),
            "source": chunk_data.get("source", "unknown"),
            "chunk_index": chunk_data.get("chunk_index", 0),
            "score": round(score, 3),
        }
        chunks.append(chunk)
        logger.debug(f"[RETRIEVER] –î–æ–±–∞–≤–ª–µ–Ω —á–∞–Ω–∫: source={chunk['source']}, score={chunk['score']}, text_length={len(chunk['text'])}")

    logger.info(f"[RETRIEVER] ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
    logger.info(f"[RETRIEVER] ===== –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ =====")
    return chunks

