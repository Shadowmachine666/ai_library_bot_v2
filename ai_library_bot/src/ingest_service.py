"""–°–µ—Ä–≤–∏—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–Ω–∏–≥ –¥–ª—è ai_library_bot.

–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –∫–Ω–∏–≥ (TXT, PDF, EPUB, FB2), —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏,
—Å–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ FAISS –∏–Ω–¥–µ–∫—Å.
"""

import hashlib
import pickle
from datetime import datetime as dt
from pathlib import Path
from typing import Any

from src.config import Config
from src.utils import run_in_executor, setup_logger
from src.category_parser import parse_categories_from_filename
from src.category_classifier import classify_book_category
from src.confirmation_manager import (
    create_confirmation_request,
    delete_confirmation_request,
    get_all_confirmations,
    get_confirmation_request,
    get_expired_requests,
    get_pending_confirmations,
    update_confirmation_status,
)
from src.admin_messages import (
    create_confirmation_keyboard,
    format_confirmation_message,
    format_success_notification_message,
)
from src.cache_utils import clear_cache
from src.pending_books_manager import remove_pending_book
from src.library_catalog import update_library_catalog

logger = setup_logger(__name__)

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ñ–∞–π–ª–æ–≤
SUPPORTED_EXTENSIONS = {".txt", ".pdf", ".epub", ".fb2"}


async def _send_notification_to_admin_direct(request: dict[str, Any]) -> int | None:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Telegram Bot API.

    –†–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∑–∞–ø—É—â–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ CLI.

    Args:
        request: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ.

    Returns:
        ID –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å.
    """
    admin_id = Config.ADMIN_TELEGRAM_ID
    if not admin_id:
        logger.warning(
            "ADMIN_TELEGRAM_ID –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ"
        )
        return None

    if not Config.TG_TOKEN:
        logger.warning(
            "TG_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ"
        )
        return None

    try:
        from telegram import Bot

        bot = Bot(token=Config.TG_TOKEN)
        message_text = format_confirmation_message(request)
        keyboard = create_confirmation_keyboard(request["request_id"])

        sent_message = await bot.send_message(
            chat_id=admin_id,
            text=message_text,
            reply_markup=keyboard,
            parse_mode="Markdown",
        )

        message_id = sent_message.message_id

        # –û–±–Ω–æ–≤–ª—è–µ–º message_id –≤ –∑–∞–ø—Ä–æ—Å–µ
        update_confirmation_status(request["request_id"], "pending", message_id)

        logger.info(
            f"[INDEXING] ‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É {admin_id} "
            f"–¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {request['request_id']} (—Ñ–∞–π–ª: {Path(request.get('file_path', '')).name})"
        )

        return message_id

    except Exception as e:
        logger.error(
            f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É {admin_id}: {e}",
            exc_info=True,
        )
        return None


async def _send_success_notification(
    book_title: str, file_path: Path, categories: list[str], chunks_count: int
) -> None:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞.

    Args:
        book_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏.
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
        chunks_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    """
    admin_id = Config.ADMIN_TELEGRAM_ID
    if not admin_id:
        logger.debug(
            "ADMIN_TELEGRAM_ID –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ"
        )
        return

    if not Config.TG_TOKEN:
        logger.debug(
            "TG_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ"
        )
        return

    try:
        from telegram import Bot

        bot = Bot(token=Config.TG_TOKEN)
        message_text = format_success_notification_message(
            book_title, file_path.name, categories, chunks_count
        )

        await bot.send_message(
            chat_id=admin_id,
            text=message_text,
            parse_mode="Markdown",
        )

        logger.info(
            f"[INDEXING] ‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É {admin_id} "
            f"–¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
        )

    except Exception as e:
        logger.warning(
            f"[INDEXING] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ "
            f"–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É {admin_id}: {e}",
            exc_info=True,
        )

# –¢–∏–ø –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤
FileIndex = dict[str, dict[str, Any]]


def _get_file_index_path() -> Path:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤.

    Returns:
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É index.files.pkl –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ FAISS –∏–Ω–¥–µ–∫—Å.
    """
    return Config.FAISS_PATH.with_suffix(".files.pkl")


@run_in_executor
def _calculate_file_hash(file_path: Path) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.

    Returns:
        SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞ –≤ –≤–∏–¥–µ hex-—Å—Ç—Ä–æ–∫–∏.
    """
    logger.debug(f"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞: {file_path}")
    sha256_hash = hashlib.sha256()
    
    try:
        with open(file_path, "rb") as f:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        
        file_hash = sha256_hash.hexdigest()
        logger.debug(f"–•–µ—à —Ñ–∞–π–ª–∞ {file_path.name}: {file_hash[:16]}...")
        return file_hash
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à —Ñ–∞–π–ª–∞: {e}") from e


def _load_file_index() -> FileIndex:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞.

    –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö:
    - file_hash: SHA256 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
    - file_size: –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
    - indexed_at: Timestamp –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (ISO format)
    - chunks_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
    - first_chunk_index: –ò–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ –≤ FAISS
    - last_chunk_index: –ò–Ω–¥–µ–∫—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞ –≤ FAISS
    - file_type: –¢–∏–ø —Ñ–∞–π–ª–∞ (.txt, .pdf, .epub, .fb2)

    Returns:
        –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (str), –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ.
        –ï—Å–ª–∏ —Ñ–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å.
    """
    index_path = _get_file_index_path()
    
    if not index_path.exists():
        logger.debug("–ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")
        return {}
    
    try:
        with open(index_path, "rb") as f:
            file_index = pickle.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤: {len(file_index)} —Ñ–∞–π–ª–æ–≤")
        return file_index
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤: {e}")
        logger.warning("–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤")
        return {}


def _save_file_index(file_index: FileIndex) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –≤ —Ñ–∞–π–ª.

    Args:
        file_index: –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö.
    """
    index_path = _get_file_index_path()
    
    try:
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        index_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(index_path, "wb") as f:
            pickle.dump(file_index, f, protocol=4)
        
        logger.info(f"–ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_path} ({len(file_index)} —Ñ–∞–π–ª–æ–≤)")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤: {e}") from e


async def _should_index_file(
    file_path: Path, file_index: FileIndex, force: bool = False
) -> tuple[bool, str, dict[str, Any] | None]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª.

    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞:
    - "new": —Ñ–∞–π–ª –Ω–æ–≤—ã–π, –Ω–µ –±—ã–ª –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω
    - "changed": —Ñ–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è (—Ö–µ—à –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
    - "unchanged": —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è (—Ö–µ—à —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
    - "not_found": —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.
        file_index: –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ (—Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö).
        force: –ï—Å–ª–∏ True, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (should_index, reason, existing_file_info):
        - should_index: True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å, False –µ—Å–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        - reason: –ü—Ä–∏—á–∏–Ω–∞ ("new", "changed", "unchanged", "not_found")
        - existing_file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º —Ñ–∞–π–ª–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ None
    """
    file_path_str = str(file_path.absolute())
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —Ñ–∏–∑–∏—á–µ—Å–∫–∏
    if not file_path.exists():
        logger.warning(f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
        return False, "not_found", None
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
    existing_file_info = file_index.get(file_path_str)
    
    # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –≤ –∏–Ω–¥–µ–∫—Å–µ - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    if existing_file_info is None:
        logger.info(f"–ù–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {file_path.name}")
        return True, "new", None
    
    # –ï—Å–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è - –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
    if force:
        logger.info(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {file_path.name}")
        return True, "changed", existing_file_info
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Ö–µ—à —Ñ–∞–π–ª–∞
    try:
        current_hash = await _calculate_file_hash(file_path)
        stored_hash = existing_file_info.get("file_hash")
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ö–µ—à–∏
        if current_hash == stored_hash:
            # –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
            logger.debug(f"–§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {file_path.name}")
            return False, "unchanged", existing_file_info
        else:
            # –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è
            logger.info(
                f"–§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {file_path.name} "
                f"(—Å—Ç–∞—Ä—ã–π —Ö–µ—à: {stored_hash[:16]}..., –Ω–æ–≤—ã–π: {current_hash[:16]}...)"
            )
            return True, "changed", existing_file_info
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –ª—É—á—à–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
        return True, "changed", existing_file_info


async def _remove_file_from_index(file_path: Path, file_index: FileIndex) -> None:
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ —Ñ–∞–π–ª–∞ –∏–∑ FAISS –∏–Ω–¥–µ–∫—Å–∞.

    –ü–æ—Å–∫–æ–ª—å–∫—É FAISS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤,
    –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç –∏–Ω–¥–µ–∫—Å –±–µ–∑ —á–∞–Ω–∫–æ–≤ —É–¥–∞–ª—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞.

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, —á–∞–Ω–∫–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å.
        file_index: –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ (–±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è).
    """
    import faiss
    import numpy as np

    file_path_str = str(file_path.absolute())
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª –≤ –∏–Ω–¥–µ–∫—Å–µ
    file_info = file_index.get(file_path_str)
    if file_info is None:
        logger.debug(f"–§–∞–π–ª {file_path.name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å–µ, –Ω–µ—á–µ–≥–æ —É–¥–∞–ª—è—Ç—å")
        return
    
    logger.info(f"–£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: {file_path.name}")
    
    index_path = Config.FAISS_PATH
    metadata_path = index_path.with_suffix(".metadata.pkl")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
    if not index_path.exists() or not metadata_path.exists():
        logger.warning("–ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–µ—á–µ–≥–æ —É–¥–∞–ª—è—Ç—å")
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤
        file_index.pop(file_path_str, None)
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    try:
        old_index = faiss.read_index(str(index_path))
        with open(metadata_path, "rb") as f:
            all_metadata = pickle.load(f)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {e}")
        logger.error("–ò–Ω–¥–µ–∫—Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∏–∑ –∏–Ω–¥–µ–∫—Å–∞")
        logger.warning("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å (—É–¥–∞–ª–∏—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–Ω–∏–≥–∏)")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–∏–Ω–¥–µ–∫—Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω): {e}") from e
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {old_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, {len(all_metadata)} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
    
    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
    chunks_to_remove = set()
    for idx, meta in enumerate(all_metadata):
        meta_file_path = meta.get("file_path", "")
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
        if str(Path(meta_file_path).absolute()) == file_path_str:
            chunks_to_remove.add(idx)
    
    if not chunks_to_remove:
        logger.warning(f"–ß–∞–Ω–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤
        file_index.pop(file_path_str, None)
        return
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(chunks_to_remove)} —á–∞–Ω–∫–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞ {file_path.name}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –≤–µ–∫—Ç–æ—Ä—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –Ω–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª—è—Ç—å)
    new_metadata = []
    vectors_to_keep = []
    
    for idx, meta in enumerate(all_metadata):
        if idx not in chunks_to_remove:
            new_metadata.append(meta)
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
            vector = old_index.reconstruct(idx)
            vectors_to_keep.append(vector)
    
    # –ï—Å–ª–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –≤–µ–∫—Ç–æ—Ä–æ–≤ - —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
    if not vectors_to_keep:
        logger.info("–í—Å–µ –≤–µ–∫—Ç–æ—Ä—ã —É–¥–∞–ª–µ–Ω—ã, —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å")
        embedding_dim = old_index.d
        new_index = faiss.IndexFlatL2(embedding_dim)
        new_metadata = []
    else:
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å –æ—Å—Ç–∞–≤—à–∏–º–∏—Å—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏
        embedding_dim = len(vectors_to_keep[0])
        new_index = faiss.IndexFlatL2(embedding_dim)
        vectors_array = np.array(vectors_to_keep, dtype=np.float32)
        new_index.add(vectors_array)
        logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {new_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤ (–±—ã–ª–æ {old_index.ntotal})")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã —á–∞–Ω–∫–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–æ–Ω–∏ —Å–¥–≤–∏–Ω—É–ª–∏—Å—å)
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ —Ñ–∞–π–ª–∞–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è first/last_chunk_index
    files_chunks: dict[str, list[int]] = {}
    for new_idx, meta in enumerate(new_metadata):
        meta_file_path = str(Path(meta.get("file_path", "")).absolute())
        if meta_file_path not in files_chunks:
            files_chunks[meta_file_path] = []
        files_chunks[meta_file_path].append(new_idx)
        # –û–±–Ω–æ–≤–ª—è–µ–º chunk_index –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        meta["chunk_index"] = new_idx
    
    # –û–±–Ω–æ–≤–ª—è–µ–º first_chunk_index –∏ last_chunk_index –≤ –∏–Ω–¥–µ–∫—Å–µ —Ñ–∞–π–ª–æ–≤
    for meta_file_path, chunk_indices in files_chunks.items():
        if chunk_indices:
            file_info = file_index.get(meta_file_path)
            if file_info:
                file_info["first_chunk_index"] = min(chunk_indices)
                file_info["last_chunk_index"] = max(chunk_indices)
                file_info["chunks_count"] = len(chunk_indices)
    
    # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å –æ —Ñ–∞–π–ª–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤
    file_index.pop(file_path_str, None)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    faiss.write_index(new_index, str(index_path))
    with open(metadata_path, "wb") as f:
        pickle.dump(new_metadata, f, protocol=4)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    _save_file_index(file_index)
    
    logger.info(
        f"–§–∞–π–ª {file_path.name} —É–¥–∞–ª—ë–Ω –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: "
        f"—É–¥–∞–ª–µ–Ω–æ {len(chunks_to_remove)} —á–∞–Ω–∫–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {new_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤"
    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–∞–ª–æ–≥ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∫–Ω–∏–≥–∏
    try:
        await update_library_catalog()
    except Exception as e:
        logger.warning(f"[INDEXING] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è: {e}")


async def _delete_file_completely(file_path: Path) -> None:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–∏–º –¥–∞–Ω–Ω—ã–µ.

    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
    2. –£–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ FAISS –∏–Ω–¥–µ–∫—Å–∞
    3. –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ file_index
    4. –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ pending_confirmations (–µ—Å–ª–∏ –µ—Å—Ç—å)

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è.

    Raises:
        ValueError: –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏.
    """
    file_path_str = str(file_path.absolute())
    logger.info(f"–ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {file_path.name}")

    # 1. –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏ file_index
    file_index = _load_file_index()
    try:
        await _remove_file_from_index(file_path, file_index)
        logger.info(f"–§–∞–π–ª {file_path.name} —É–¥–∞–ª—ë–Ω –∏–∑ –∏–Ω–¥–µ–∫—Å–∞")
    except Exception as e:
        logger.warning(
            f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name} –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: {e}. "
            f"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã."
        )

    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ pending_confirmations (–µ—Å–ª–∏ –µ—Å—Ç—å)
    # –ò—â–µ–º –∑–∞–ø—Ä–æ—Å –ø–æ file_path
    all_confirmations = get_all_confirmations()
    for request_id, request in all_confirmations.items():
        request_file_path = request.get("file_path", "")
        if str(Path(request_file_path).absolute()) == file_path_str:
            try:
                delete_confirmation_request(request_id)
                logger.info(
                    f"–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {request_id} —É–¥–∞–ª—ë–Ω –¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
                )
            except Exception as e:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ {request_id}: {e}"
                )
            break

    # 3. –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
    if file_path.exists():
        try:
            file_path.unlink()
            logger.info(f"–§–∞–π–ª {file_path.name} —É–¥–∞–ª—ë–Ω –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name} –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã: {e}")
            raise ValueError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path.name} –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã: {e}"
            ) from e
    else:
        logger.warning(f"–§–∞–π–ª {file_path.name} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ")

    logger.info(f"‚úÖ –§–∞–π–ª {file_path.name} –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—ë–Ω –∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")


async def check_and_cleanup_expired_confirmations() -> int:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å—Ç—ë–∫—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã.

    –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Ç–µ–∫–ª–∏ –ø–æ —Ç–∞–π–º–∞—É—Ç—É, –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Ö —Å—Ç–∞—Ç—É—Å
    –Ω–∞ "timeout" –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª—è–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.

    Returns:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
    """
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç—ë–∫—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ...")

    expired_request_ids = get_expired_requests()

    if not expired_request_ids:
        logger.debug("–ò—Å—Ç—ë–∫—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return 0

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(expired_request_ids)} –∏—Å—Ç—ë–∫—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")

    deleted_count = 0

    for request_id in expired_request_ids:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å
            request = get_confirmation_request(request_id)
            if not request:
                logger.warning(f"–ó–∞–ø—Ä–æ—Å {request_id} –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            file_path_str = request.get("file_path", "")
            if not file_path_str:
                logger.warning(f"–ó–∞–ø—Ä–æ—Å {request_id} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç file_path, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∏ —É–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                update_confirmation_status(request_id, "timeout")
                delete_confirmation_request(request_id)
                continue

            file_path = Path(file_path_str)
            book_title = request.get("book_title", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

            logger.info(
                f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç—ë–∫—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ {request_id} –¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
            )

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "timeout"
            update_confirmation_status(request_id, "timeout")

            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
            try:
                await _delete_file_completely(file_path)
                deleted_count += 1
                logger.info(
                    f"‚úÖ –§–∞–π–ª {file_path.name} —É–¥–∞–ª—ë–Ω –∏–∑-–∑–∞ –∏—Å—Ç–µ—á–µ–Ω–∏—è —Ç–∞–π–º–∞—É—Ç–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"
                )
            except Exception as e:
                logger.error(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name} –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {request_id}: {e}",
                    exc_info=True,
                )
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

            # –£–¥–∞–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∏–∑ pending_confirmations
            delete_confirmation_request(request_id)

        except Exception as e:
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç—ë–∫—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ {request_id}: {e}",
                exc_info=True,
            )
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    logger.info(
        f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç—ë–∫—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {deleted_count} —Ñ–∞–π–ª–æ–≤"
    )

    return deleted_count


@run_in_executor
def _read_txt_file(file_path: Path) -> str:
    """–ß–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª.

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.

    Returns:
        –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞.
    """
    logger.debug(f"–ß—Ç–µ–Ω–∏–µ TXT —Ñ–∞–π–ª–∞ {file_path}")
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size = file_path.stat().st_size
    logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ {file_path.name}: {file_size} –±–∞–π—Ç")
    
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    encodings = ["utf-8", "utf-8-sig", "cp1251", "windows-1251", "latin-1"]
    for encoding in encodings:
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ –±–∏–Ω–∞—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º
            with open(file_path, "rb") as f:
                raw_content = f.read()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ null-–±–∞–π—Ç–æ–≤ –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if b'\x00' in raw_content:
                logger.warning(f"–§–∞–π–ª {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç null-–±–∞–π—Ç—ã, —É–¥–∞–ª—è–µ–º –∏—Ö")
                raw_content = raw_content.replace(b'\x00', b'')
            
            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ —Ñ–∞–π–ª –≤ cp1251, –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            # Python —Å—Ç—Ä–æ–∫–∏ —Ö—Ä–∞–Ω—è—Ç —Ç–µ–∫—Å—Ç –≤ Unicode, –Ω–æ –Ω—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ
            if encoding in ["cp1251", "windows-1251"]:
                # –î–ª—è cp1251: –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –±–∞–π—Ç—ã –Ω–∞–ø—Ä—è–º—É—é –≤ Unicode —Å—Ç—Ä–æ–∫—É
                # Python –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç cp1251 –≤ Unicode –ø—Ä–∏ decode
                content = raw_content.decode("cp1251", errors="replace")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã, –∑–Ω–∞—á–∏—Ç —Ñ–∞–π–ª –Ω–µ –≤ cp1251
                preview = content[:100]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
                has_cyrillic = any('\u0400' <= c <= '\u04FF' for c in preview)
                if not has_cyrillic and len(preview) > 10:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—ã, –Ω–æ —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π, –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞
                    logger.warning(f"–§–∞–π–ª {file_path.name} –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å {encoding}, –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—ã. –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É.")
                    continue
            else:
                content = raw_content.decode(encoding, errors="replace")
            
            logger.info(f"–§–∞–π–ª {file_path.name} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}, –¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∞–∫–æ–∑—è–±—Ä
            preview = content[:100]
            if "—Å–ø–µ–∫—É–ª—è—Ü–∏—è" in preview.lower() or "–°–ø–µ–∫—É–ª—è—Ü–∏—è" in preview:
                logger.info(f"‚úÖ –§–∞–π–ª {file_path.name} —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–æ '—Å–ø–µ–∫—É–ª—è—Ü–∏—è' –≤ –Ω–∞—á–∞–ª–µ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é
            if len(content) < 100:
                logger.warning(f"–§–∞–π–ª {file_path.name} –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤). –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π.")
            elif file_size > 0 and len(content) < file_size / 2:
                logger.warning(f"–§–∞–π–ª {file_path.name} –ø—Ä–æ—á–∏—Ç–∞–Ω, –Ω–æ –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤) –Ω–∞–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ ({file_size} –±–∞–π—Ç). –í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            
            return content
        except (UnicodeDecodeError, LookupError) as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {file_path.name} —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}: {e}")
            continue
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ, –ø—Ä–æ–±—É–µ–º —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
    with open(file_path, "rb") as f:
        raw_content = f.read()
    # –£–¥–∞–ª—è–µ–º null-–±–∞–π—Ç—ã
    raw_content = raw_content.replace(b'\x00', b'')
    content = raw_content.decode("utf-8", errors="replace")
    logger.warning(f"–§–∞–π–ª {file_path.name} –ø—Ä–æ—á–∏—Ç–∞–Ω —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫, –¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
    return content


@run_in_executor
def _read_pdf_file(file_path: Path) -> str:
    """–ß–∏—Ç–∞–µ—Ç PDF —Ñ–∞–π–ª.

    Args:
        file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É.

    Returns:
        –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ PDF.

    Raises:
        ValueError: –ï—Å–ª–∏ PDF —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª—å—à–µ MAX_PDF_PAGES —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    import PyPDF2

    logger.debug(f"–ß—Ç–µ–Ω–∏–µ PDF —Ñ–∞–π–ª–∞ {file_path}")

    try:
        with open(file_path, "rb") as file:
            reader = PyPDF2.PdfReader(file)
            
            num_pages = len(reader.pages)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            if num_pages > Config.MAX_PDF_PAGES:
                raise ValueError(
                    f"PDF —Å–æ–¥–µ—Ä–∂–∏—Ç {num_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, "
                    f"–º–∞–∫—Å–∏–º—É–º —Ä–∞–∑—Ä–µ—à–µ–Ω–æ {Config.MAX_PDF_PAGES}"
                )
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö PDF (–±–æ–ª—å—à–µ 500 —Å—Ç—Ä–∞–Ω–∏—Ü)
            if num_pages > 500:
                file_name = Path(file_path).name
                logger.warning(
                    f"‚ö†Ô∏è PDF —Ñ–∞–π–ª {file_name} —Å–æ–¥–µ—Ä–∂–∏—Ç {num_pages} —Å—Ç—Ä–∞–Ω–∏—Ü "
                    f"(–±–æ–ª—å—à–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö 500). –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏."
                )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            text_parts = []
            for page_num, page in enumerate(reader.pages, 1):
                try:
                    text = page.extract_text()
                    if text.strip():
                        text_parts.append(text)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                    continue
            
            content = "\n\n".join(text_parts)
            file_name = Path(file_path).name
            logger.info(
                f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {num_pages} —Å—Ç—Ä–∞–Ω–∏—Ü PDF —Ñ–∞–π–ª–∞ {file_name}"
            )
            return content
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å PDF —Ñ–∞–π–ª: {e}") from e


@run_in_executor
def _read_epub_file(file_path: Path) -> str:
    """–ß–∏—Ç–∞–µ—Ç EPUB —Ñ–∞–π–ª.

    Args:
        file_path: –ü—É—Ç—å –∫ EPUB —Ñ–∞–π–ª—É.

    Returns:
        –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ EPUB.
    """
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup

    logger.debug(f"–ß—Ç–µ–Ω–∏–µ EPUB —Ñ–∞–π–ª–∞ {file_path}")

    try:
        book = epub.read_epub(str(file_path))
        text_parts = []
        
        for item in book.get_items():
            if item.get_type() == ebooklib.ITEM_DOCUMENT:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ HTML
                soup = BeautifulSoup(item.get_content(), "html.parser")
                text = soup.get_text(separator="\n", strip=True)
                if text.strip():
                    text_parts.append(text)
        
        content = "\n\n".join(text_parts)
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ EPUB")
        return content
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ EPUB —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å EPUB —Ñ–∞–π–ª: {e}") from e


@run_in_executor
def _read_fb2_file(file_path: Path) -> str:
    """–ß–∏—Ç–∞–µ—Ç FB2 —Ñ–∞–π–ª.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ XML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã FB2,
    –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ EPUB —Ñ–∞–π–ª–æ–≤.

    Args:
        file_path: –ü—É—Ç—å –∫ FB2 —Ñ–∞–π–ª—É.

    Returns:
        –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ FB2.
    """
    from bs4 import BeautifulSoup

    logger.debug(f"–ß—Ç–µ–Ω–∏–µ FB2 —Ñ–∞–π–ª–∞ {file_path}")

    try:
        # FB2 - —ç—Ç–æ XML —Ñ–æ—Ä–º–∞—Ç, —á–∏—Ç–∞–µ–º –∫–∞–∫ XML
        with open(file_path, "rb") as f:
            raw_content = f.read()
        
        # –ü–∞—Ä—Å–∏–º XML —Å –ø–æ–º–æ—â—å—é BeautifulSoup (–∫–∞–∫ –¥–ª—è EPUB)
        soup = BeautifulSoup(raw_content, "xml")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ body
        text_parts = []
        bodies = soup.find_all("body")
        
        for body in bodies:
            sections = body.find_all("section", recursive=True)
            if not sections:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–∫—Ü–∏–π, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é –∏–∑ body
                body_text = body.get_text(separator="\n", strip=True)
                if body_text:
                    text_parts.append(body_text)
            else:
                for section in sections:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ —Å–µ–∫—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    section_text = section.get_text(separator="\n", strip=True)
                    if section_text:
                        text_parts.append(section_text)
        
        content = "\n\n".join(text_parts)
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ FB2")
        
        if not content.strip():
            logger.warning(f"FB2 —Ñ–∞–π–ª {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
        
        return content
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ FB2 —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å FB2 —Ñ–∞–π–ª: {e}") from e


def _extract_metadata(file_path: Path, content: str) -> dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∞–≤—Ç–æ—Ä, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏).

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏: title, author, file_path, topics.
    """
    logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}")

    # –ü–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    book_title, categories = parse_categories_from_filename(file_path)

    # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF/EPUB/FB2
    return {
        "title": book_title,  # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        "author": "Unknown",  # –ê–≤—Ç–æ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        "file_path": str(file_path),
        "file_type": file_path.suffix.lower(),
        "topics": categories,  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏)
    }


def _chunk_text(
    text: str, chunk_size: int | None = None, chunk_overlap: int | None = None
) -> list[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏.

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è.
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ Config.
        chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ Config.

    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞.
    """
    if chunk_size is None:
        chunk_size = Config.CHUNK_SIZE
    if chunk_overlap is None:
        chunk_overlap = Config.CHUNK_OVERLAP

    logger.debug(f"–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏: —Ä–∞–∑–º–µ—Ä={chunk_size}, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ={chunk_overlap}")

    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = start + chunk_size
        chunk = text[start:end]

        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –º–µ–Ω—å—à–µ MIN_CHUNK_SIZE
        if len(chunk.strip()) >= Config.MIN_CHUNK_SIZE:
            chunks.append(chunk.strip())

        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —á–∞–Ω–∫—É —Å —É—á—ë—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        start = end - chunk_overlap
        if start >= text_length:
            break

    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {text_length} —Å–∏–º–≤–æ–ª–æ–≤")
    return chunks


async def _create_embeddings_batch(texts: list[str]) -> list[list[float]]:
    """–°–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤.

    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

    Returns:
        –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–∞–∂–¥—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ - —Å–ø–∏—Å–æ–∫ float).

    Raises:
        ValueError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.
    """
    if not Config.OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    from openai import AsyncOpenAI

    client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
    
    logger.debug(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI API")
    
    try:
        response = await client.embeddings.create(
            model=Config.EMBEDDING_MODEL,
            input=texts
        )
        embeddings = [item.embedding for item in response.data]
        logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        return embeddings
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {e}") from e


async def _rebuild_index_from_metadata() -> tuple[bool, str]:
    """–í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
    
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —á–∞–Ω–∫–æ–≤ –∏–∑ index.metadata.pkl, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å. –≠—Ç–æ –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è.
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—É—Å–ø–µ—Ö, —Å–æ–æ–±—â–µ–Ω–∏–µ). –ï—Å–ª–∏ —É—Å–ø–µ—Ö True, –∏–Ω–¥–µ–∫—Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.
    """
    import faiss
    import numpy as np
    
    index_path = Config.FAISS_PATH
    metadata_path = index_path.with_suffix(".metadata.pkl")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    if not metadata_path.exists():
        return False, "–§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ"
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        with open(metadata_path, "rb") as f:
            all_metadata = pickle.load(f)
        
        if not all_metadata:
            return False, "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —á–∞–Ω–∫–æ–≤
        chunks_texts = []
        valid_metadata = []
        for meta in all_metadata:
            chunk_text = meta.get("chunk_text", "")
            if chunk_text and isinstance(chunk_text, str):
                chunks_texts.append(chunk_text)
                valid_metadata.append(meta)
            else:
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —á–∞–Ω–∫ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞: {meta.get('source', 'unknown')}")
        
        if not chunks_texts:
            return False, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —á–∞–Ω–∫–æ–≤ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks_texts)} —á–∞–Ω–∫–æ–≤ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
        logger.info(f"üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks_texts)} —á–∞–Ω–∫–æ–≤...")
        all_embeddings = []
        batch_size = Config.EMBEDDING_BATCH_SIZE
        total_batches = (len(chunks_texts) + batch_size - 1) // batch_size
        
        for i in range(0, len(chunks_texts), batch_size):
            batch_num = (i // batch_size) + 1
            batch = chunks_texts[i : i + batch_size]
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} ({len(batch)} —á–∞–Ω–∫–æ–≤)")
            batch_embeddings = await _create_embeddings_batch(batch)
            all_embeddings.extend(batch_embeddings)
        
        if len(all_embeddings) != len(chunks_texts):
            return False, f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –æ–∂–∏–¥–∞–ª–æ—Å—å {len(chunks_texts)}, –ø–æ–ª—É—á–µ–Ω–æ {len(all_embeddings)}"
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
        
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å
        embedding_dim = len(all_embeddings[0])
        embeddings_array = np.array(all_embeddings, dtype=np.float32)
        index = faiss.IndexFlatL2(embedding_dim)
        index.add(embeddings_array)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        faiss.write_index(index, str(index_path))
        logger.info(f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_path} ({index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –æ–Ω–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ –∏–Ω–¥–µ–∫—Å—É
        with open(metadata_path, "wb") as f:
            pickle.dump(valid_metadata, f, protocol=4)
        logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {len(valid_metadata)} –∑–∞–ø–∏—Å–µ–π")
        
        return True, f"–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(chunks_texts)} —á–∞–Ω–∫–æ–≤, {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤"
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}"
        logger.error(f"‚ùå {error_msg}")
        return False, error_msg


async def _save_to_faiss(
    embeddings: list[list[float]],
    chunks: list[str],
    metadata: list[dict[str, Any]],
    file_path: Path,
    file_hash: str,
    file_index: FileIndex | None = None,
) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ FAISS –∏–Ω–¥–µ–∫—Å.

    –¢–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ.

    Args:
        embeddings: –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        chunks: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤.
        metadata: –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞.
        file_path: –ü—É—Ç—å –∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É.
        file_hash: SHA256 —Ö–µ—à —Ñ–∞–π–ª–∞.
        file_index: –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è. –ï—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
    """
    import faiss
    import numpy as np
    import pickle

    if not embeddings:
        logger.warning("–ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    embedding_dim = len(embeddings[0])
    logger.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ FAISS –∏–Ω–¥–µ–∫—Å (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding_dim})")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array
    embeddings_array = np.array(embeddings, dtype=np.float32)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
    index_path = Config.FAISS_PATH
    metadata_path = index_path.with_suffix(".metadata.pkl")

    if index_path.exists():
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
        try:
            index = faiss.read_index(str(index_path))
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å —Å {index.ntotal} –≤–µ–∫—Ç–æ—Ä–∞–º–∏")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if metadata_path.exists():
                try:
                    with open(metadata_path, "rb") as f:
                        all_metadata = pickle.load(f)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
                    logger.warning("–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å, —Ç–∞–∫ –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã")
                    index = faiss.IndexFlatL2(embedding_dim)
                    all_metadata = []
            else:
                all_metadata = []
        except Exception as e:
            # –ò–Ω–¥–µ–∫—Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            logger.warning("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
            import shutil
            backup_suffix = dt.now().strftime("%Y%m%d_%H%M%S")
            try:
                corrupted_index_path = index_path.parent / f"index.faiss.corrupted_{backup_suffix}"
                shutil.move(str(index_path), str(corrupted_index_path))
                logger.info(f"–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫: {corrupted_index_path.name}")
            except Exception as backup_error:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {backup_error}")
                # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å
                try:
                    index_path.unlink()
                except Exception:
                    pass
            
            # –ü—ã—Ç–∞–µ–º—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            success, message = await _rebuild_index_from_metadata()
            if success:
                logger.info(f"‚úÖ {message}")
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                index = faiss.read_index(str(index_path))
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if metadata_path.exists():
                    with open(metadata_path, "rb") as f:
                        all_metadata = pickle.load(f)
                else:
                    all_metadata = []
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å {index.ntotal} –≤–µ–∫—Ç–æ—Ä–∞–º–∏")
            else:
                # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
                logger.error(f"‚ùå {message}")
                logger.warning("‚ö†Ô∏è –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å")
                logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ –∏–Ω–¥–µ–∫—Å–µ –±—É–¥—É—Ç –ø–æ—Ç–µ—Ä—è–Ω—ã!")
                logger.warning("‚ö†Ô∏è –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫–Ω–∏–≥–∏ –∑–∞–Ω–æ–≤–æ")
                index = faiss.IndexFlatL2(embedding_dim)
                all_metadata = []
                logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embedding_dim}")
    else:
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        index = faiss.IndexFlatL2(embedding_dim)
        all_metadata = []
        logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embedding_dim}")

    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞ (–¥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è)
    first_chunk_index = len(all_metadata)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    index.add(embeddings_array)
    all_metadata.extend(metadata)

    # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–∞–Ω–∫–∞ (–ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è)
    last_chunk_index = len(all_metadata) - 1
    chunks_count = len(embeddings)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    faiss.write_index(index, str(index_path))
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª pickle 4 –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ UTF-8
    with open(metadata_path, "wb") as f:
        pickle.dump(all_metadata, f, protocol=4)

    logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_path} ({index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, {len(all_metadata)} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)")

    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    if file_index is None:
        file_index = _load_file_index()
    
    file_path_str = str(file_path.absolute())
    file_size = file_path.stat().st_size
    file_type = file_path.suffix.lower()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –≤ –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    file_index[file_path_str] = {
        "file_hash": file_hash,
        "file_size": file_size,
        "indexed_at": dt.now().isoformat(),
        "chunks_count": chunks_count,
        "first_chunk_index": first_chunk_index,
        "last_chunk_index": last_chunk_index,
        "file_type": file_type,
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    _save_file_index(file_index)
    
    logger.info(
        f"–§–∞–π–ª {file_path.name} –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤: "
        f"{chunks_count} —á–∞–Ω–∫–æ–≤ (–∏–Ω–¥–µ–∫—Å—ã {first_chunk_index}-{last_chunk_index})"
    )


async def _determine_categories(
    file_path: Path,
    book_title: str,
    categories_from_filename: list[str],
    content_preview: str | None = None,
) -> list[str] | None:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–Ω–∏–≥–∏.

    –õ–æ–≥–∏–∫–∞:
    1. –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    2. –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ—Ç - –≤—ã–∑—ã–≤–∞–µ–º LLM –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    3. –ï—Å–ª–∏ LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª - —Å–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
    4. –ï—Å–ª–∏ LLM –Ω–µ —Å–º–æ–≥ - —Å–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        book_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏.
        categories_from_filename: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
        content_preview: –ü–µ—Ä–≤—ã–µ —Å–∏–º–≤–æ–ª—ã —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∫–Ω–∏–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ LLM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).

    Returns:
        –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–ª–∏ None, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞.
    """
    # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    if categories_from_filename:
        logger.info(
            f"[INDEXING] ‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {categories_from_filename}"
        )
        return categories_from_filename

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –≤—ã–∑—ã–≤–∞–µ–º LLM
    logger.info(
        f"[INDEXING] –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {file_path.name}, "
        f"–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —á–µ—Ä–µ–∑ LLM..."
    )
    if content_preview:
        logger.debug(
            f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ "
            f"({len(content_preview)} —Å–∏–º–≤–æ–ª–æ–≤)"
        )

    try:
        llm_result = await classify_book_category(book_title, content_preview)
        llm_categories = llm_result.get("topics", [])
        llm_confidence = llm_result.get("confidence", 0.0)
        llm_reasoning = llm_result.get("reasoning", "")

        if llm_categories:
            logger.info(
                f"[INDEXING] LLM –æ–ø—Ä–µ–¥–µ–ª–∏–ª –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è '{book_title}': {llm_categories} "
                f"(confidence: {llm_confidence:.2f})"
            )

            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            request_id = create_confirmation_request(
                file_path=file_path,
                book_title=book_title,
                categories_from_filename=[],
                categories_llm_recommendation=llm_categories,
                llm_confidence=llm_confidence,
                llm_reasoning=llm_reasoning,
            )

            logger.info(
                f"[INDEXING] –°–æ–∑–¥–∞–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {request_id} "
                f"–¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
            )

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É
            request_data = get_confirmation_request(request_id)
            if request_data:
                await _send_notification_to_admin_direct(request_data)
            else:
                logger.warning(
                    f"[INDEXING] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞ {request_id} "
                    f"–¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"
                )

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None - —Ñ–∞–π–ª –Ω–µ –±—É–¥–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –¥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
            return None
        else:
            logger.warning(
                f"[INDEXING] LLM –Ω–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è '{book_title}'"
            )

            # –°–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            request_id = create_confirmation_request(
                file_path=file_path,
                book_title=book_title,
                categories_from_filename=[],
                categories_llm_recommendation=[],
                llm_confidence=0.0,
                llm_reasoning="LLM –Ω–µ —Å–º–æ–≥ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏",
            )

            logger.info(
                f"[INDEXING] –°–æ–∑–¥–∞–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–±–µ–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏): {request_id} "
                f"–¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
            )

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É
            request_data = get_confirmation_request(request_id)
            if request_data:
                await _send_notification_to_admin_direct(request_data)
            else:
                logger.warning(
                    f"[INDEXING] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞ {request_id} "
                    f"–¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"
                )

            return None

    except Exception as e:
        logger.error(
            f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ LLM –¥–ª—è '{book_title}': {e}",
            exc_info=True,
        )

        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Å –æ—à–∏–±–∫–æ–π
        request_id = create_confirmation_request(
            file_path=file_path,
            book_title=book_title,
            categories_from_filename=[],
            categories_llm_recommendation=[],
            llm_confidence=0.0,
            llm_reasoning=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {str(e)}",
        )

        logger.info(
            f"[INDEXING] –°–æ–∑–¥–∞–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—Å –æ—à–∏–±–∫–æ–π): {request_id} "
            f"–¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
        )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É
        request_data = get_confirmation_request(request_id)
        if request_data:
            await _send_notification_to_admin_direct(request_data)
        else:
            logger.warning(
                f"[INDEXING] ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å–∞ {request_id} "
                f"–¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è"
            )

        return None


async def _process_file(
    file_path: Path, file_index: FileIndex | None = None
) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª: —á–∏—Ç–∞–µ—Ç, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏, —Å–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç.

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        file_index: –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è. –ï—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.

    Raises:
        ValueError: –ï—Å–ª–∏ —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –∏–ª–∏ –∏–º–µ–µ—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç.
    """
    logger.info(f"[INDEXING] ===== –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_path.name} =====")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    if file_size_mb > Config.MAX_FILE_SIZE_MB:
        raise ValueError(
            f"–§–∞–π–ª {file_path} —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size_mb:.2f} MB "
            f"(–º–∞–∫—Å–∏–º—É–º {Config.MAX_FILE_SIZE_MB} MB)"
        )

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
    if file_path.suffix.lower() not in SUPPORTED_EXTENSIONS:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path.suffix}")

    # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    file_hash = await _calculate_file_hash(file_path)

    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
    extension = file_path.suffix.lower()
    if extension == ".txt":
        content = await _read_txt_file(file_path)  # type: ignore[misc]
    elif extension == ".pdf":
        content = await _read_pdf_file(file_path)  # type: ignore[misc]
    elif extension == ".epub":
        content = await _read_epub_file(file_path)  # type: ignore[misc]
    elif extension == ".fb2":
        content = await _read_fb2_file(file_path)  # type: ignore[misc]
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {extension}")

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    logger.info(f"[INDEXING] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞: {file_path.name}")
    metadata_base = _extract_metadata(file_path, content)
    categories_from_filename = metadata_base.get("topics", [])
    book_title = metadata_base.get("title", "")
    
    logger.info(
        f"[INDEXING] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã: "
        f"–Ω–∞–∑–≤–∞–Ω–∏–µ='{book_title}', "
        f"–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞={categories_from_filename}, "
        f"–¥–ª–∏–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ={len(content)} —Å–∏–º–≤–æ–ª–æ–≤"
    )

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤)
    content_preview = content[:2000].strip() if content else None
    if content_preview:
        logger.debug(
            f"[INDEXING] –ü—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {len(content_preview)} —Å–∏–º–≤–æ–ª–æ–≤"
        )

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    logger.info(f"[INDEXING] –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ñ–∞–π–ª–∞: {file_path.name}")
    final_categories = await _determine_categories(
        file_path,
        book_title,
        categories_from_filename,
        content_preview,
    )

    # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã (–Ω—É–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ), –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ñ–∞–π–ª
    if final_categories is None:
        logger.info(
            f"[INDEXING] ‚è∏Ô∏è –§–∞–π–ª {file_path.name} –Ω–µ –±—É–¥–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω: "
            f"—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º"
        )
        logger.info(f"[INDEXING] ===== –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {file_path.name} –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ =====\n")
        return

    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    await _continue_indexing_with_categories(
        file_path, file_index, metadata_base, content, file_hash, final_categories
    )


async def _continue_indexing_with_categories(
    file_path: Path,
    file_index: FileIndex | None,
    metadata_base: dict[str, Any],
    content: str,
    file_hash: str,
    categories: list[str],
) -> None:
    """–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Ñ–∞–π–ª–∞ —Å —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º.

    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        file_index: –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.
        metadata_base: –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞.
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞.
        file_hash: –•–µ—à —Ñ–∞–π–ª–∞.
        categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ñ–∞–π–ª–∞.
    """
    logger.info(
        f"[INDEXING] –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name} —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏: {categories}"
    )

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata_base["topics"] = categories
    logger.info(
        f"[INDEXING] ‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã: {categories}"
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
    if metadata_base.get("topics") != categories:
        logger.error(
            f"[INDEXING] ‚ùå –û–®–ò–ë–ö–ê: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ! "
            f"–û–∂–∏–¥–∞–ª–æ—Å—å: {categories}, –ø–æ–ª—É—á–µ–Ω–æ: {metadata_base.get('topics')}"
        )
    else:
        logger.debug(
            f"[INDEXING] ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_base.get('topics')}"
        )

    # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
    logger.info(
        f"[INDEXING] –†–∞–∑–±–∏–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ {file_path.name} –Ω–∞ —á–∞–Ω–∫–∏ "
        f"(–¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤)"
    )
    chunks = _chunk_text(content)
    logger.info(
        f"[INDEXING] ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ {file_path.name}"
    )

    if not chunks:
        logger.warning(
            f"–§–∞–π–ª {file_path} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–≤—Å–µ –º–µ–Ω—å—à–µ {Config.MIN_CHUNK_SIZE} —Å–∏–º–≤–æ–ª–æ–≤)"
        )
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–∞—Ç—á–∞–º–∏
    logger.info(
        f"[INDEXING] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤ "
        f"(—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {Config.EMBEDDING_BATCH_SIZE})"
    )
    all_embeddings = []
    batch_size = Config.EMBEDDING_BATCH_SIZE
    total_batches = (len(chunks) + batch_size - 1) // batch_size

    for i in range(0, len(chunks), batch_size):
        batch_num = (i // batch_size) + 1
        batch = chunks[i : i + batch_size]
        logger.debug(
            f"[INDEXING] –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}/{total_batches} "
            f"({len(batch)} —á–∞–Ω–∫–æ–≤)"
        )
        batch_embeddings = await _create_embeddings_batch(batch)
        all_embeddings.extend(batch_embeddings)
        logger.debug(
            f"[INDEXING] –ë–∞—Ç—á {batch_num}/{total_batches} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: "
            f"—Å–æ–∑–¥–∞–Ω–æ {len(batch_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
        )
    
    logger.info(
        f"[INDEXING] ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(all_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}"
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    logger.info(
        f"[INDEXING] –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤"
    )
    chunks_metadata = []
    categories_check_failed = 0
    
    for idx, chunk in enumerate(chunks):
        chunk_meta = metadata_base.copy()
        chunk_meta["chunk_index"] = idx
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ UTF-8 –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        # chunk —É–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –ø–æ—Å–ª–µ _read_txt_file, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º
        if isinstance(chunk, bytes):
            # –ï—Å–ª–∏ —ç—Ç–æ bytes, –ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            try:
                chunk = chunk.decode("utf-8")
            except UnicodeDecodeError:
                try:
                    chunk = chunk.decode("cp1251")
                except UnicodeDecodeError:
                    chunk = chunk.decode("utf-8", errors="replace")
        elif not isinstance(chunk, str):
            chunk = str(chunk)
        
        # –ü—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å - Python –∏ pickle –¥–æ–ª–∂–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å UTF-8
        # –ì–ª–∞–≤–Ω–æ–µ - —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Ç—Ä–æ–∫–∞, –∞ –Ω–µ bytes
        chunk_meta["chunk_text"] = chunk
        # –î–æ–±–∞–≤–ª—è–µ–º source –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞)
        chunk_meta["source"] = file_path.name
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
        if chunk_meta.get("topics") != categories:
            categories_check_failed += 1
            logger.error(
                f"[INDEXING] ‚ùå –û–®–ò–ë–ö–ê: –ß–∞–Ω–∫ {idx} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏! "
                f"–û–∂–∏–¥–∞–ª–æ—Å—å: {categories}, –ø–æ–ª—É—á–µ–Ω–æ: {chunk_meta.get('topics')}"
            )
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            chunk_meta["topics"] = categories
            logger.warning(
                f"[INDEXING] ‚ö†Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –¥–ª—è —á–∞–Ω–∫–∞ {idx}"
            )
        
        chunks_metadata.append(chunk_meta)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∞–∫–æ–∑—è–±—Ä (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        preview = chunk[:100]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–µ –±—É–∫–≤—ã, –Ω–µ —Ü–∏—Ñ—Ä—ã, –Ω–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, –Ω–µ –ø—Ä–æ–±–µ–ª—ã, –Ω–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)
        unreadable_count = sum(1 for c in preview if ord(c) > 127 and not c.isprintable() and c not in "\n\r\t" and c not in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø")
        if unreadable_count > 10:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 10 –Ω–µ—á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            logger.warning(
                f"[INDEXING] ‚ö†Ô∏è –ß–∞–Ω–∫ {idx} –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {preview[:50]}..."
            )
    
    if categories_check_failed > 0:
        logger.error(
            f"[INDEXING] ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {categories_check_failed} —á–∞–Ω–∫–æ–≤ "
            f"–Ω–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)"
        )
    else:
        logger.info(
            f"[INDEXING] ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: –≤—Å–µ {len(chunks_metadata)} —á–∞–Ω–∫–æ–≤ "
            f"—Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {categories}"
        )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if file_index is None:
        file_index = _load_file_index()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ FAISS —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤
    await _save_to_faiss(
        all_embeddings, chunks, chunks_metadata, file_path, file_hash, file_index
    )

    logger.info(
        f"[INDEXING] ‚úÖ –§–∞–π–ª {file_path.name} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: "
        f"{len(chunks)} —á–∞–Ω–∫–æ–≤, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {categories}"
    )
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –æ–± —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    book_title = metadata_base.get("title", file_path.stem)
    await _send_success_notification(
        book_title, file_path, categories, len(chunks)
    )
    
    # –ö–∞—Ç–∞–ª–æ–≥ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω –≤ –∫–æ–Ω—Ü–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–≤ ingest_books)
    # –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: –æ–¥–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
    
    logger.info(f"[INDEXING] ===== –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {file_path.name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ =====\n")


async def continue_indexing_after_confirmation(request_id: str) -> bool:
    """–ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º.

    Args:
        request_id: ID –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ.

    Returns:
        True –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∞, False –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    from src.confirmation_manager import get_confirmation_request

    logger.info(f"[INDEXING] –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {request_id}")

    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    request = get_confirmation_request(request_id)
    if not request:
        logger.error(f"[INDEXING] ‚ùå –ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω: {request_id}")
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—Ä–æ—Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω
    if request.get("status") != "approved":
        logger.warning(
            f"[INDEXING] ‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å {request_id} –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω (—Å—Ç–∞—Ç—É—Å: {request.get('status')})"
        )
        return False

    file_path = Path(request.get("file_path", ""))
    if not file_path.exists():
        logger.error(f"[INDEXING] ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return False

    # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    categories = request.get("categories_llm_recommendation", [])
    if not categories:
        categories = request.get("categories_from_filename", [])

    if not categories:
        logger.error(f"[INDEXING] ‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ {request_id}")
        return False

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
        file_index = _load_file_index()

        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        extension = file_path.suffix.lower()
        if extension == ".txt":
            content = await _read_txt_file(file_path)  # type: ignore[misc]
        elif extension == ".pdf":
            content = await _read_pdf_file(file_path)  # type: ignore[misc]
        elif extension == ".epub":
            content = await _read_epub_file(file_path)  # type: ignore[misc]
        elif extension == ".fb2":
            content = await _read_fb2_file(file_path)  # type: ignore[misc]
        else:
            logger.error(f"[INDEXING] ‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {extension}")
            return False

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_base = _extract_metadata(file_path, content)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞
        file_hash = await _calculate_file_hash(file_path)

        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        await _continue_indexing_with_categories(
            file_path, file_index, metadata_base, content, file_hash, categories
        )

        logger.info(f"[INDEXING] ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {file_path.name}")
        return True

    except Exception as e:
        logger.error(
            f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {request_id}: {e}",
            exc_info=True
        )
        return False


async def check_for_new_books(folder_path: str) -> list[Path]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤—ã—Ö –∫–Ω–∏–≥ –≤ –ø–∞–ø–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã.
    
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ —Å –∏–Ω–¥–µ–∫—Å–æ–º –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç
    —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ pending_books.
    
    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∫–Ω–∏–≥–∞–º–∏.
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –Ω–æ–≤—ã–º —Ñ–∞–π–ª–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã.
    """
    folder = Path(folder_path)
    
    if not folder.exists() or not folder.is_dir():
        logger.warning(f"[NEW_BOOKS_CHECK] –ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π: {folder_path}")
        return []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    file_index = _load_file_index()
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
    files_in_folder: list[Path] = []
    for ext in SUPPORTED_EXTENSIONS:
        files_in_folder.extend(folder.glob(f"*{ext}"))
        files_in_folder.extend(folder.glob(f"*{ext.upper()}"))
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    files_in_folder = list(dict.fromkeys(files_in_folder))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –Ω–æ–≤—ã–µ (–Ω–µ –≤ –∏–Ω–¥–µ–∫—Å–µ)
    new_files: list[Path] = []
    
    for file_path in files_in_folder:
        file_path_str = str(file_path.absolute())
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç –≤ –∏–Ω–¥–µ–∫—Å–µ - —ç—Ç–æ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        if file_path_str not in file_index:
            new_files.append(file_path)
            logger.debug(f"[NEW_BOOKS_CHECK] –ù–∞–π–¥–µ–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª: {file_path.name}")
    
    if new_files:
        logger.info(f"[NEW_BOOKS_CHECK] –ù–∞–π–¥–µ–Ω–æ {len(new_files)} –Ω–æ–≤—ã—Ö –∫–Ω–∏–≥ –≤ –ø–∞–ø–∫–µ {folder_path}")
    else:
        logger.debug(f"[NEW_BOOKS_CHECK] –ù–æ–≤—ã—Ö –∫–Ω–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–ø–∫–µ {folder_path}")
    
    return new_files


async def ingest_books(folder_path: str, force: bool = False) -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–Ω–∏–≥ –∏–∑ –ø–∞–ø–∫–∏.

    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ:
    - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –Ω—É–∂–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å (–Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ)
    - –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    - –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    - –£–¥–∞–ª—è–µ—Ç –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –∏–∑ –ø–∞–ø–∫–∏

    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∫–Ω–∏–≥–∞–º–∏.
        force: –ï—Å–ª–∏ True, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å.

    Raises:
        FileNotFoundError: –ï—Å–ª–∏ –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        ValueError: –ï—Å–ª–∏ –≤ –ø–∞–ø–∫–µ –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤.
    """
    folder = Path(folder_path)

    if not folder.exists():
        raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")

    if not folder.is_dir():
        raise ValueError(f"–£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–∞–ø–∫–æ–π: {folder_path}")

    logger.info(f"[INDEXING] ===== –ù–ê–ß–ê–õ–û –ò–ù–î–ï–ö–°–ê–¶–ò–ò –ö–ù–ò–ì =====")
    logger.info(f"[INDEXING] –ü–∞–ø–∫–∞: {folder_path}")
    if force:
        logger.info("[INDEXING] ‚ö†Ô∏è –†–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: –≤—Å–µ —Ñ–∞–π–ª—ã –±—É–¥—É—Ç –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤
    file_index = _load_file_index()
    if file_index:
        logger.info(f"[INDEXING] –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤: {len(file_index)} —Ñ–∞–π–ª–æ–≤")
    else:
        logger.info("[INDEXING] –ò–Ω–¥–µ–∫—Å —Ñ–∞–π–ª–æ–≤ –ø—É—Å—Ç, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π")

    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
    files_in_folder: list[Path] = []
    for ext in SUPPORTED_EXTENSIONS:
        files_in_folder.extend(folder.glob(f"*{ext}"))
        files_in_folder.extend(folder.glob(f"*{ext.upper()}"))
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–∞–π–¥–µ–Ω –∏ —Å .txt –∏ —Å .TXT)
    files_in_folder = list(dict.fromkeys(files_in_folder))  # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫

    if not files_in_folder and not file_index:
        logger.warning(f"[INDEXING] ‚ö†Ô∏è –í –ø–∞–ø–∫–µ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç")
        return

    logger.info(f"[INDEXING] –ù–∞–π–¥–µ–Ω–æ {len(files_in_folder)} —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å
    files_to_index: list[Path] = []  # –§–∞–π–ª—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    files_to_remove: list[Path] = []  # –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å (–Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏)
    files_skipped = 0  # –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å

    for file_path in files_in_folder:
        should_index, reason, existing_info = await _should_index_file(file_path, file_index, force)
        
        if should_index:
            if reason == "changed" and existing_info:
                # –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è - –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π
                files_to_remove.append(file_path)
            files_to_index.append(file_path)
            logger.info(f"[INDEXING] –ù–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {file_path.name}")
            logger.info(f"[INDEXING] –§–∞–π–ª {file_path.name}: {reason} ‚Üí –±—É–¥–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
        else:
            files_skipped += 1
            logger.debug(f"[INDEXING] –§–∞–π–ª {file_path.name}: {reason} ‚Üí –ø—Ä–æ–ø—É—â–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–¥–∞–ª—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–µ—Å—Ç—å –≤ –∏–Ω–¥–µ–∫—Å–µ, –Ω–æ –Ω–µ—Ç –≤ –ø–∞–ø–∫–µ)
    folder_abs = folder.absolute()
    files_in_folder_abs = {str(f.absolute()) for f in files_in_folder}
    files_to_delete_from_index: list[str] = []
    
    for indexed_file_path_str in file_index.keys():
        indexed_file_path = Path(indexed_file_path_str)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ
        try:
            if indexed_file_path.parent.absolute() == folder_abs:
                if indexed_file_path_str not in files_in_folder_abs:
                    # –§–∞–π–ª –±—ã–ª –≤ –∏–Ω–¥–µ–∫—Å–µ, –Ω–æ –µ–≥–æ –Ω–µ—Ç –≤ –ø–∞–ø–∫–µ
                    files_to_delete_from_index.append(indexed_file_path_str)
        except Exception:
            # –ï—Å–ª–∏ –ø—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue

    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
    files_were_deleted = False
    if files_to_delete_from_index:
        logger.info(f"[INDEXING] –£–¥–∞–ª–µ–Ω–∏–µ {len(files_to_delete_from_index)} —Ñ–∞–π–ª–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ (—Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã –∏–∑ –ø–∞–ø–∫–∏)")
        for file_path_str in files_to_delete_from_index:
            file_path = Path(file_path_str)
            logger.info(f"[INDEXING] –§–∞–π–ª {file_path.name} —É–¥–∞–ª—ë–Ω –∏–∑ –ø–∞–ø–∫–∏, —É–¥–∞–ª—è–µ–º –∏–∑ –∏–Ω–¥–µ–∫—Å–∞")
            try:
                await _remove_file_from_index(file_path, file_index)
                files_were_deleted = True
            except Exception as e:
                logger.error(f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name} –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: {e}")

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if files_to_remove:
        logger.info(f"[INDEXING] –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è {len(files_to_remove)} –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        for file_path in files_to_remove:
            logger.info(f"[INDEXING] –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑–º–µ–Ω—ë–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {file_path.name}")
            try:
                await _remove_file_from_index(file_path, file_index)
                files_were_deleted = True
            except Exception as e:
                logger.error(f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ä—ã—Ö —á–∞–Ω–∫–æ–≤ —Ñ–∞–π–ª–∞ {file_path.name}: {e}")

    # –û—á–∏—â–∞–µ–º –∫—ç—à –æ—Ç–≤–µ—Ç–æ–≤ LLM –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤/—á–∞–Ω–∫–æ–≤
    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –æ—Ç–≤–µ—Ç—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∞—Ö
    if files_were_deleted:
        logger.info("[INDEXING] –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç–≤–µ—Ç–æ–≤ LLM –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤/—á–∞–Ω–∫–æ–≤")
        await clear_cache()

    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    processed = 0
    errors = 0
    pending_confirmation = 0

    if files_to_index:
        logger.info(f"[INDEXING] –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é {len(files_to_index)} —Ñ–∞–π–ª–æ–≤\n")
        for file_path in files_to_index:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
                before_confirmation_count = len(get_pending_confirmations())
                await _process_file(file_path, file_index)
                after_confirmation_count = len(get_pending_confirmations())
                
                # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —É–≤–µ–ª–∏—á–∏–ª–æ—Å—å, —Ñ–∞–π–ª —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
                if after_confirmation_count > before_confirmation_count:
                    pending_confirmation += 1
                else:
                    processed += 1
                    # –£–¥–∞–ª—è–µ–º –∫–Ω–∏–≥—É –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                    remove_pending_book(file_path)
                    logger.debug(f"[INDEXING] –ö–Ω–∏–≥–∞ {file_path.name} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            except Exception as e:
                errors += 1
                logger.error(
                    f"[INDEXING] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}",
                    exc_info=True
                )
    else:
        logger.info("[INDEXING] –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")

    # –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    final_pending = len(get_pending_confirmations())

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"[INDEXING] ===== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–î–ï–ö–°–ê–¶–ò–ò =====")
    logger.info(f"[INDEXING] –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {len(files_in_folder)}")
    logger.info(f"[INDEXING] ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {processed} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"[INDEXING] ‚è∏Ô∏è –¢—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {pending_confirmation} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"[INDEXING] ‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å): {files_skipped} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"[INDEXING] üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: {len(files_to_delete_from_index)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"[INDEXING] ‚ùå –û—à–∏–±–æ–∫: {errors} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"[INDEXING] üìã –í—Å–µ–≥–æ –æ–∂–∏–¥–∞–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {final_pending} —Ñ–∞–π–ª–æ–≤")
    
    if final_pending > 0:
        logger.info(
            f"[INDEXING] ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: {final_pending} —Ñ–∞–π–ª–æ–≤ –æ–∂–∏–¥–∞—é—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π. "
            f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /pending –≤ –±–æ—Ç–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞."
        )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–∞–ª–æ–≥ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    # (–µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è: –¥–æ–±–∞–≤–ª–µ–Ω—ã, —É–¥–∞–ª–µ–Ω—ã –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω—ã –∫–Ω–∏–≥–∏)
    if processed > 0 or files_were_deleted or len(files_to_delete_from_index) > 0:
        try:
            await update_library_catalog()
        except Exception as e:
            logger.warning(f"[INDEXING] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
    
    logger.info(f"[INDEXING] ===== –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê =====\n")
